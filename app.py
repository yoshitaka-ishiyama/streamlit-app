import streamlit as st

# ============================
# Step0: ã‚¢ãƒ—ãƒªåˆæœŸåŒ–ï¼ˆStreamlitè¨­å®šï¼‰
# ============================
st.set_page_config(page_title="RAG Chatbot (TF-IDF + Gemini)", layout="centered")

# ============================
# Step0: import
# ============================
import os
import re
import pandas as pd
import numpy as np

from dotenv import load_dotenv
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai

# ============================
# Step0: æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå›ºå®šã§OKï¼‰
# ============================
ALPHA = 0.6          # çµ±åˆã‚¹ã‚³ã‚¢ã®é‡ã¿ï¼šwordå¯„ã‚Š=1.0 / charå¯„ã‚Š=0.0
TOP_K = 3            # å‚ç…§ã™ã‚‹è¨˜äº‹æ•°
MIN_SCORE = 0.05     # ã“ã‚Œæœªæº€ã¯ã€Œãƒ’ãƒƒãƒˆãªã—ã€æ‰±ã„ï¼ˆåºƒã™ãã‚‹è³ªå•ã®èª¤ãƒ’ãƒƒãƒˆæŠ‘åˆ¶ï¼‰

# ============================
# Step0: .env èª­ã¿è¾¼ã¿ â†’ APIã‚­ãƒ¼ç¢ºèª
# ============================
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    st.error("ç’°å¢ƒå¤‰æ•° GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ============================
# Step1: Gemini APIï¼ˆLLMï¼‰æº–å‚™
# ============================
@st.cache_resource
def get_gemini_model(api_key: str):
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-2.5-flash-lite")

def gemini_ask(model, prompt: str) -> str:
    prompt = (prompt or "").strip()
    if not prompt:
        return ""
    resp = model.generate_content(prompt)
    return getattr(resp, "text", "") or ""

# ============================
# Step2: CSVèª­è¾¼ â†’ docs ä½œæˆ
# ============================
@st.cache_data
def load_news_csv(csv_path: str) -> pd.DataFrame:
    """
    ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹CSVã‚’èª­ã¿è¾¼ã¿ã€åˆ†æç”¨ã«å‰å‡¦ç†ã—ãŸDataFrameã‚’è¿”ã™ã€‚

    å½¹å‰²ï¼š
    1. CSVã®èª­ã¿è¾¼ã¿
    2. å¿…é ˆã‚«ãƒ©ãƒ ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¹ã‚­ãƒ¼ãƒä¿è¨¼ï¼‰
    3. æ¬ æå€¤(NaN)ã‚’ç©ºæ–‡å­—ã«çµ±ä¸€ï¼ˆå¾Œç¶šã®TF-IDF / æ¤œç´¢å‡¦ç†ã§ã®ã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰
    4. Streamlitã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚Šå†èª­ã¿è¾¼ã¿é«˜é€ŸåŒ–

    Parameters
    ----------
    csv_path : str
        èª­ã¿è¾¼ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    Returns
    -------
    pd.DataFrame
        å‰å‡¦ç†æ¸ˆã¿ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
    """

    # CSVèª­ã¿è¾¼ã¿
    df = pd.read_csv(csv_path)

    # ===== ã‚¹ã‚­ãƒ¼ãƒãƒã‚§ãƒƒã‚¯ =====
    # å¿…é ˆã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯å³ã‚¨ãƒ©ãƒ¼ï¼ˆé™ã‹ã«å£Šã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹ï¼‰
    required_cols = ["topic", "url", "title", "text", "text_tokenized"]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        raise ValueError(f"CSVã«å¿…è¦ãªåˆ—ãŒã‚ã‚Šã¾ã›ã‚“: {missing}. columns={list(df.columns)}")

    # ===== æ¬ æå€¤å‡¦ç† =====
    # TF-IDF / æ–‡å­—åˆ—å‡¦ç†ã§NaNãŒã‚ã‚‹ã¨è½ã¡ã‚‹ãŸã‚ã€ç©ºæ–‡å­—ã«çµ±ä¸€
    df["topic"] = df["topic"].fillna("")
    df["url"] = df["url"].fillna("")
    df["title"] = df["title"].fillna("")
    df["text"] = df["text"].fillna("")
    df["text_tokenized"] = df["text_tokenized"].fillna("")

    return df

def build_docs(df: pd.DataFrame) -> list[dict]:
    """
    ãƒ‹ãƒ¥ãƒ¼ã‚¹DataFrameã‚’ã€æ¤œç´¢ãƒ»RAGç”¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ§‹é€ ï¼ˆlist[dict]ï¼‰ã«å¤‰æ›ã™ã‚‹ã€‚

    å½¹å‰²ï¼š
    1. å„è¨˜äº‹ã‚’ã€Œ1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€ã¨ã—ã¦æ•´ç†
    2. TF-IDFæ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆï¼ˆword / charï¼‰ã‚’æº–å‚™
    3. Geminiã«æ¸¡ã™è¡¨ç¤ºç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
    4. å¾Œç¶šå‡¦ç†ã§æ‰±ã„ã‚„ã™ã„è¾æ›¸å½¢å¼ã«çµ±ä¸€

    Parameters
    ----------
    df : pd.DataFrame
        å‰å‡¦ç†æ¸ˆã¿ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿

    Returns
    -------
    list[dict]
        æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ç”¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé…åˆ—
    """

    docs: list[dict] = []

    # reset_indexã§ 0,1,2... ã®é€£ç•ªIDã‚’æŒ¯ã‚Šç›´ã™
    for i, row in df.reset_index(drop=True).iterrows():

        # å‰å¾Œç©ºç™½é™¤å»ï¼ˆæ¤œç´¢ç²¾åº¦å®‰å®šåŒ–ï¼‰
        title = str(row["title"]).strip()
        text = str(row["text"]).strip()

        docs.append(
            {
                # ä¸€æ„ãªIDï¼ˆæ¤œç´¢çµæœâ†’å…ƒè¨˜äº‹å‚ç…§ç”¨ï¼‰
                "doc_id": int(i),

                # ãƒ¡ã‚¿æƒ…å ±
                "topic": str(row["topic"]),
                "url": str(row["url"]),
                "title": title,
                "text": text,

                # ===== TF-IDFç”¨ =====

                # å˜èªãƒ™ãƒ¼ã‚¹æ¤œç´¢ç”¨ï¼ˆCSVå´ã§åˆ†ã‹ã¡æ›¸ãæ¸ˆã¿ï¼‰
                "tfidf_word": str(row["text_tokenized"]),

                # æ–‡å­—n-gramæ¤œç´¢ç”¨ï¼ˆè‡ªç„¶æ–‡ãƒ»è³ªå•æ–‡ã«å¼·ã„ï¼‰
                # ã‚¿ã‚¤ãƒˆãƒ«ã‚‚æ··ãœã‚‹ã“ã¨ã§ãƒ’ãƒƒãƒˆç‡ã‚¢ãƒƒãƒ—
                "tfidf_char": f"{title} {text}",

                # ===== Gemini(RAG)ç”¨ =====
                # LLMã«ãã®ã¾ã¾æ¸¡ã›ã‚‹æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆ
                "display_text": f"ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘{title}\n\nã€æœ¬æ–‡ã€‘\n{text}",
            }
        )

    return docs

# ============================
# Step3: TF-IDF ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆwordç‰ˆ / charç‰ˆï¼‰
# ============================
@st.cache_data
def build_tfidf_indexes(docs: list[dict]):
    """
    æ¤œç´¢ç”¨TF-IDFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹é–¢æ•°ã€‚

    å½¹å‰²ï¼š
    1. è¨˜äº‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆæ•°å€¤åŒ–ï¼‰
    2. é¡ä¼¼åº¦è¨ˆç®—ï¼ˆcosine similarityï¼‰å¯èƒ½ãªå½¢ã«å¤‰æ›
    3. wordæ¤œç´¢ + charæ¤œç´¢ ã®2ç³»çµ±ã‚’ç”¨æ„ã—ã¦ç²¾åº¦å‘ä¸Š

    æˆ»ã‚Šå€¤ï¼š
        word_vectorizer : å˜èªãƒ™ãƒ¼ã‚¹æ¤œç´¢ç”¨ã®TF-IDFãƒ¢ãƒ‡ãƒ«
        word_matrix     : å˜èªãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—ï¼ˆè¨˜äº‹Ã—å˜èªï¼‰
        char_vectorizer : æ–‡å­—n-gramæ¤œç´¢ç”¨TF-IDFãƒ¢ãƒ‡ãƒ«
        char_matrix     : æ–‡å­—ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—ï¼ˆè¨˜äº‹Ã—æ–‡å­—n-gramï¼‰
    """

    # =========================
    # wordï¼ˆåˆ†ã‹ã¡æ›¸ãæ¤œç´¢ï¼‰
    # =========================

    # ä¾‹: "æ—¥éŠ€ æ”¿ç­– é‡‘åˆ© æ®ãˆç½®ã"
    word_corpus = [d["tfidf_word"] for d in docs]

    # å˜èªã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã‚’ãã®ã¾ã¾ä½¿ã†è¨­å®š
    word_vectorizer = TfidfVectorizer(
        tokenizer=str.split,   # ç©ºç™½åŒºåˆ‡ã‚Š
        preprocessor=None,     # è¿½åŠ å‰å‡¦ç†ãªã—
        token_pattern=None,    # sklearnæ¨™æº–åˆ†å‰²ã‚’ç„¡åŠ¹åŒ–
    )

    # å­¦ç¿’ + ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆã“ã“ã§ã€Œæ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ç”Ÿæˆï¼‰
    word_matrix = word_vectorizer.fit_transform(word_corpus)

    # =========================
    # charï¼ˆæ–‡å­—n-gramæ¤œç´¢ï¼‰
    # =========================

    # ã‚¿ã‚¤ãƒˆãƒ« + æœ¬æ–‡ã®è‡ªç„¶æ–‡
    char_corpus = [d["tfidf_char"] for d in docs]

    char_vectorizer = TfidfVectorizer(
        analyzer="char_wb",   # æ–‡å­—n-gramè§£æï¼ˆå˜èªå¢ƒç•Œè€ƒæ…®ï¼‰
        ngram_range=(2, 4),   # 2ã€œ4æ–‡å­—å˜ä½
        min_df=2,            # 2è¨˜äº‹ä»¥ä¸Šã«å‡ºç¾ã™ã‚‹èªã ã‘ä½¿ç”¨ï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰
    )

    char_matrix = char_vectorizer.fit_transform(char_corpus)

    return word_vectorizer, word_matrix, char_vectorizer, char_matrix

# ============================
# Step3.5: è³ªå•æ–‡ã‚’TF-IDFå‘ã‘ã«ç°¡æ˜“æ­£è¦åŒ–ï¼ˆæ±ç”¨ç‰ˆï¼‰
# - vocabï¼ˆwordå´ã®èªå½™ï¼‰ã«å¯„ã›ã‚‹
# ============================
# ============================
# ã‚¯ã‚¨ãƒªæ­£è¦åŒ–ï¼ˆè‡ªç„¶æ–‡ â†’ TF-IDFæ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
# ============================

# æ—¥æœ¬èªã‚’ã–ã£ãã‚Šã€Œæ–‡å­—ç¨®ã”ã¨ã®å¡Šã€ã«åˆ†å‰²ã™ã‚‹æ­£è¦è¡¨ç¾
# ä¾‹ï¼š
# ã€Œæ—¥éŠ€ã®æ”¿ç­–é‡‘åˆ©ã«ã¤ã„ã¦æ•™ãˆã¦ã€
# â†’ ["æ—¥éŠ€", "ã®", "æ”¿ç­–é‡‘åˆ©", "ã«ã¤ã„ã¦", "æ•™ãˆã¦"]
_CHUNK_RE = re.compile(r"[ä¸€-é¾¥]+|[ã-ã‚“]+|[ã‚¡-ãƒ³]+|[A-Za-z0-9]+")


def normalize_query_for_tfidf(query: str, vocab: set[str]) -> str:
    """
    è‡ªç„¶æ–‡ã®è³ªå•ã‚’ã€ŒTF-IDFæ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã€ã«å¤‰æ›ã™ã‚‹é–¢æ•°ã€‚

    ç›®çš„ï¼š
    - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç„¶æ–‡è³ªå•ã‚’ã€CSV(text_tokenized)ã¨åŒã˜èªå½™å½¢å¼ã«æƒãˆã‚‹
    - å½¢æ…‹ç´ è§£æãªã—ã§è»½é‡ã«å®Ÿè£…
    - æ¤œç´¢ã«ä¸è¦ãªåŠ©è©ãƒ»å®šå‹èªã‚’é™¤å»
    - TF-IDFèªå½™(vocab)ã«å­˜åœ¨ã™ã‚‹èªã®ã¿æ®‹ã™ï¼ˆèª¤ãƒ’ãƒƒãƒˆé˜²æ­¢ï¼‰

    ä¾‹ï¼š
        å…¥åŠ› : "æ—¥éŠ€ã®æ”¿ç­–é‡‘åˆ©ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„"
        å‡ºåŠ› : "æ—¥éŠ€ æ”¿ç­– é‡‘åˆ©"

    æˆ»ã‚Šå€¤ï¼š
        ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ–‡å­—åˆ—
    """

    # None/ç©ºå¯¾ç­–
    q = (query or "").strip()
    if not q:
        return ""

    # =========================
    # â‘  è¨˜å·é™¤å»
    # =========================
    # è¨˜å·ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›ï¼ˆæ¤œç´¢ãƒã‚¤ã‚ºé˜²æ­¢ï¼‰
    q = re.sub(r"[^\wã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]+", " ", q)


    # =========================
    # â‘¡ ã–ã£ãã‚Šåˆ†å‰²ï¼ˆç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼‰
    # =========================
    # æ¼¢å­—/ã²ã‚‰ãŒãª/ã‚«ã‚¿ã‚«ãƒŠ/è‹±æ•°å­—ã®å¡Šã”ã¨ã«åˆ†å‰²
    chunks = _CHUNK_RE.findall(q)


    # =========================
    # â‘¢ ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
    # =========================
    # æ„å‘³ã‚’æŒãŸãªã„èªï¼ˆåŠ©è©ãƒ»å®šå‹èªï¼‰ã‚’å‰Šé™¤
    stop = {
        "ã«ã¤ã„ã¦","æ•™ãˆã¦","ãã ã•ã„","ã¨ã¯","ã§ã™","ã¾ã™","ã™ã‚‹","ã—ãŸã„",
        "ã®","ãŒ","ã‚’","ã«","ã¸","ã¨","ã‚‚","ã‚„","ã‹ã‚‰","ã¾ã§","ã‚ˆã‚Š",
        "ã§ã™ã‹","ã¾ã™ã‹","ã‚ã‚‹","ã‚ã‚Šã¾ã™","ã„ã‚‹","ã„ã¾ã™",
        "æœ€è¿‘","ä½•","ã©ã‚“ãª","ã©ã†"
    }

    candidates = []
    for c in chunks:
        if c in stop:
            continue
        candidates.append(c)


    # =========================
    # â‘£ è¤‡åˆèªã®åˆ†å‰²è£œåŠ©
    # =========================
    # ä¾‹: æ”¿ç­–é‡‘åˆ© â†’ æ”¿ç­– + é‡‘åˆ©
    expanded = []

    for t in candidates:
        expanded.append(t)

        # vocabã«ç„¡ã„é•·ã„èªã ã‘åˆ†å‰²æ¢ç´¢
        if (t not in vocab) and (len(t) >= 4):
            for i in range(2, len(t) - 1):
                a, b = t[:i], t[i:]
                if (a in vocab) and (b in vocab):
                    expanded.extend([a, b])


    # =========================
    # â‘¤ vocabã«ã‚ã‚‹èªã ã‘æ®‹ã™ï¼ˆæœ€é‡è¦ï¼‰
    # =========================
    # æ¤œç´¢å¯¾è±¡èªå½™ä»¥å¤–ã¯å‰Šé™¤ â†’ ç²¾åº¦å®‰å®š
    expanded = [t for t in expanded if t in vocab]


    # =========================
    # â‘¥ é‡è¤‡é™¤å»
    # =========================
    seen = set()
    uniq = []
    for t in expanded:
        if t not in seen:
            uniq.append(t)
            seen.add(t)


    # æœ€çµ‚çš„ã«ã€Œã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šæ–‡å­—åˆ—ã€ã«å¤‰æ›
    return " ".join(uniq)

# ============================
# Step4: æ¤œç´¢ï¼ˆçµ±åˆã‚¹ã‚³ã‚¢ã§ä¸Šä½æŠ½å‡ºï¼‰
# ============================
def search_docs(
    user_query: str,
    docs: list[dict],
    word_vectorizer,
    word_matrix,
    char_vectorizer,
    char_matrix,
    vocab: set[str],
    top_k: int = 3,
    alpha: float = 0.6,
    min_score: float = 0.05,
):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•(user_query)ã«å¯¾ã—ã¦ã€TF-IDFé¡ä¼¼æ¤œç´¢ã§ä¸Šä½è¨˜äº‹ã‚’è¿”ã™ã€‚

    ã­ã‚‰ã„ï¼š
    - wordç‰ˆTF-IDFï¼šCSVå´ã®åˆ†ã‹ã¡æ›¸ã(text_tokenized)ã‚’æ´»ç”¨ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¼·ã„ï¼‰
    - charç‰ˆTF-IDFï¼šè‡ªç„¶æ–‡ã‚„è¡¨è¨˜ã‚†ã‚Œã«å¼·ã„ï¼ˆè³ªå•æ–‡ã‚’ãã®ã¾ã¾æŠ•ã’ã‚‰ã‚Œã‚‹ï¼‰
    - çµ±åˆã‚¹ã‚³ã‚¢ï¼šcombined = alpha * word + (1-alpha) * char
      ãŸã ã—ã€wordå´ã®ã‚¯ã‚¨ãƒªãŒä½œã‚Œãªã„ï¼ˆèªå½™ã«ä¹—ã‚‰ãªã„ï¼‰å ´åˆã¯ã€charã®ã¿ã‚’æ¡ç”¨ã—ã¦è–„ã‚ãªã„

    æˆ»ã‚Šå€¤ï¼š
    - results: ä¸Šä½è¨˜äº‹ã®ãƒªã‚¹ãƒˆï¼ˆdoc_id/title/url/topic/score/display_textï¼‰
    - debug:   æ¤œç´¢ã«ä½¿ã£ãŸq_wordï¼ˆwordå´ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ï¼‰ã‚„max_scoreãªã©
    """
    # 0) å…¥åŠ›ãƒã‚§ãƒƒã‚¯ï¼šç©ºãªã‚‰æ¤œç´¢ã—ãªã„
    user_query = (user_query or "").strip()
    if not user_query:
        return [], {"q_word": "", "max_score": 0.0}

    # 1) wordç”¨ã®ã‚¯ã‚¨ãƒªã‚’ä½œã‚‹ï¼ˆèªå½™å¯„ã›ï¼‰
    #    - è‡ªç„¶æ–‡ â†’ vocabã«å­˜åœ¨ã™ã‚‹èªã ã‘ã‚’æ®‹ã—ãŸã€Œã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã€
    #    - ã“ã‚Œã«ã‚ˆã‚Šã€word TF-IDFãŒæ„å‘³ã®ã‚ã‚‹å…¥åŠ›ã‚’å—ã‘å–ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
    q_word = normalize_query_for_tfidf(user_query, vocab)

    # 2) charæ¤œç´¢ï¼ˆè‡ªç„¶æ–‡ã®ã¾ã¾ï¼‰ã‚’å¸¸ã«å®Ÿæ–½ï¼šè¡¨è¨˜ã‚†ã‚Œ/è‡ªç„¶æ–‡ã®ä¿é™º
    qv_char = char_vectorizer.transform([user_query])
    char_scores = cosine_similarity(qv_char, char_matrix).flatten()

    # 3) wordãŒä½œã‚ŒãŸå ´åˆã®ã¿ã€wordæ¤œç´¢ã‚‚å®Ÿæ–½ã—çµ±åˆã‚¹ã‚³ã‚¢ã¸
    if q_word:
        qv_word = word_vectorizer.transform([q_word])
        word_scores = cosine_similarity(qv_word, word_matrix).flatten()

        # çµ±åˆã‚¹ã‚³ã‚¢ï¼ˆalphaã§é‡ã¿ä»˜ã‘ï¼‰
        combined = alpha * word_scores + (1 - alpha) * char_scores
    else:
        # wordãŒä½œã‚Œãªã„ï¼èªå½™ã«å¯„ã‚‰ãªã„è‡ªç„¶æ–‡
        # â†’ alphaã§è–„ã‚ã‚‹ã¨ç²¾åº¦ãŒè½ã¡ã‚‹ã®ã§ã€charã‚’100%æ¡ç”¨
        combined = char_scores

    # 4) ä¸Šä½Kä»¶ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—ï¼ˆé™é †ï¼‰
    top_idx = np.argsort(combined)[::-1][:top_k]

    # 5) é–¾å€¤æœªæº€ã¯æ¨ã¦ã‚‹ï¼ˆåºƒã™ãã‚‹è³ªå•ã®èª¤ãƒ’ãƒƒãƒˆã‚’æŠ‘åˆ¶ï¼‰
    top_idx = [i for i in top_idx if combined[i] >= min_score]

    # 6) çµæœæ•´å½¢ï¼šdocsã‹ã‚‰å¿…è¦æƒ…å ±ã ã‘ã‚’æŠœãå‡ºã—ã¦è¿”ã™
    results = []
    for i in top_idx:
        results.append(
            {
                "doc_id": docs[i]["doc_id"],
                "title": docs[i]["title"],
                "url": docs[i]["url"],
                "topic": docs[i]["topic"],
                "score": float(combined[i]),
                "display_text": docs[i]["display_text"],  # Geminiã«æ¸¡ã™æ ¹æ‹ 
            }
        )

    # 7) ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆå­¦ç¿’ãƒ»æ¤œè¨¼ç”¨ï¼‰
    debug = {
        "q_word": q_word,
        "max_score": float(combined.max()) if combined.size else 0.0,
    }

    return results, debug


# ============================
# Step5: RAGï¼ˆæ ¹æ‹ ä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
# ============================
def build_rag_prompt(question: str, results: list[dict], query_keywords: str) -> str:
    """
    æ¤œç´¢ã§ãƒ’ãƒƒãƒˆã—ãŸè¨˜äº‹(results)ã‚’ã€Œæ ¹æ‹ ã€ã¨ã—ã¦ã€Geminiã«æ¸¡ã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œã‚‹ã€‚

    ã­ã‚‰ã„ï¼š
    - æ ¹æ‹ ï¼ˆè¨˜äº‹æœ¬æ–‡ï¼‰ä»¥å¤–ã¯æ¨æ¸¬ã—ãªã„ã‚ˆã†ã«å³å¯†ã«æŒ‡ç¤º
    - æ ¹æ‹ ç•ªå· [æ ¹æ‹ 1], [æ ¹æ‹ 2]... ã‚’ä»˜ã‘ã€å›ç­”ã®æœ€å¾Œã«å‚ç…§ç•ªå·ã‚’å‡ºã•ã›ã‚‹
    - display_textã¯é•·ããªã‚ŠãŒã¡ãªã®ã§ã€å„æ ¹æ‹ ã‚’æœ€å¤§2000æ–‡å­—ã«åˆ¶é™ï¼ˆã‚³ã‚¹ãƒˆ/æš´èµ°é˜²æ­¢ï¼‰
    """
    evidence_blocks = []

    # 1) æ ¹æ‹ ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½œã‚‹ï¼ˆä¸Šä½Kä»¶åˆ†ï¼‰
    for i, r in enumerate(results, start=1):
        evidence = (r.get("display_text") or "")[:2000]  # é•·ã™ãã‚‹ã¨LLMãŒæ‰±ã„ã¥ã‚‰ã„ã®ã§åˆ¶é™
        evidence_blocks.append(
            f"[æ ¹æ‹ {i}] topic={r.get('topic')} url={r.get('url')}\n{evidence}"
        )

    # 2) æ ¹æ‹ ã‚’åŒºåˆ‡ã‚Šç·šã§çµåˆ
    evidence_text = "\n\n---\n\n".join(evidence_blocks)

    # 3) Geminiå‘ã‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ¨æ¸¬ç¦æ­¢ã‚’æ˜è¨˜ï¼‰
    prompt = f"""
ã‚ãªãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®è¦ç´„ãƒ»è§£èª¬ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æ¬¡ã®ã€æ ¹æ‹ ã€‘ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã ã‘ã‚’ä½¿ã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
æ ¹æ‹ ã«æ›¸ã‹ã‚Œã¦ã„ãªã„ã“ã¨ã¯æ¨æ¸¬ã›ãšã€ã€Œæ ¹æ‹ ä¸è¶³ã€ã¨æ˜ç¢ºã«ä¼ãˆã¦ãã ã•ã„ã€‚

# è³ªå•
{question}

# è³ªå•ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå‚è€ƒï¼‰
{query_keywords}

# æ ¹æ‹ 
{evidence_text}

# æŒ‡ç¤º
- ã¾ãšçµè«–ã‚’1ã€œ2è¡Œ
- æ¬¡ã«æ ¹æ‹ ã«åŸºã¥ãèª¬æ˜ï¼ˆç®‡æ¡æ›¸ãã§OKï¼‰
- æœ€å¾Œã«ã€Œå‚ç…§ã—ãŸæ ¹æ‹ ç•ªå·ï¼ˆä¾‹ï¼š[æ ¹æ‹ 1][æ ¹æ‹ 3]ï¼‰ã€ã‚’ä»˜ã‘ã‚‹
""".strip()

    return prompt


# ============================
# Step2ã€œStep5: èµ·å‹•æ™‚ã«ä¸€åº¦ã ã‘æº–å‚™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚åŠ¹ãï¼‰
# ============================
# ã“ã“ã¯Streamlitå®Ÿè¡Œæ™‚ã«æ¯å›ä¸Šã‹ã‚‰è©•ä¾¡ã•ã‚Œã‚‹ãŒã€
# @st.cache_data / @st.cache_resource ã‚’ä»˜ã‘ã¦ã„ã‚‹ã®ã§å†…éƒ¨çš„ã«ã¯å†åˆ©ç”¨ã•ã‚Œã‚„ã™ã„

CSV_PATH = "dataset/yahoo_news_articles_preprocessed.csv"

# 1) CSVèª­ã¿è¾¼ã¿ï¼ˆå¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯ãƒ»æ¬ æè£œå®Œï¼‰
df_news = load_news_csv(CSV_PATH)

# 2) DataFrame â†’ docsï¼ˆæ¤œç´¢ãƒ»æ ¹æ‹ æç¤ºã§ä½¿ã†è¾æ›¸ãƒªã‚¹ãƒˆï¼‰
docs = build_docs(df_news)

# 3) TF-IDFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆword/charï¼‰
word_vec, word_mat, char_vec, char_mat = build_tfidf_indexes(docs)

# 4) Geminiãƒ¢ãƒ‡ãƒ«æº–å‚™ï¼ˆAPIã‚­ãƒ¼ã§åˆæœŸåŒ–ï¼‰
gemini_model = get_gemini_model(GOOGLE_API_KEY)

# â˜…é‡è¦ï¼šwordå´ã®èªå½™ï¼ˆvocabï¼‰ã‚’ä½œã‚‹
# normalize_query_for_tfidf() ãŒ vocab ã‚’å‚ç…§ã—ã€è³ªå•æ–‡ã‚’èªå½™ã«å¯„ã›ãŸãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«ã™ã‚‹
VOCAB = set(word_vec.get_feature_names_out())


# ============================
# Step6: UIï¼ˆãƒãƒ£ãƒƒãƒˆï¼‰
# ============================
# ç”»é¢ã‚¿ã‚¤ãƒˆãƒ«ã¨å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¡¨ç¤º
st.title("ğŸ—¨ï¸ ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ãƒãƒ£ãƒƒãƒˆï¼ˆTF-IDF + Gemini RAGï¼‰")
st.caption(f"æ¤œç´¢è¨­å®šï¼ˆå›ºå®šï¼‰ï¼šalpha={ALPHA} / TOP_K={TOP_K} / MIN_SCORE={MIN_SCORE}")

# ä¼šè©±å±¥æ­´ã‚’ä¿æŒã™ã‚‹ï¼ˆStreamlitã¯rerunã•ã‚Œã‚‹ã®ã§ session_state ã‚’ä½¿ã†ï¼‰
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ—¢å­˜ã®å±¥æ­´ã‚’æç”»
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# å…¥åŠ›æ¬„
user_input = st.chat_input("ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šæ—¥éŠ€ã®æ”¿ç­–é‡‘åˆ©ã«ã¤ã„ã¦æ•™ãˆã¦ï¼‰")

if user_input:
    # 1) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "user", "content": user_input})

    # 2) Retrievalï¼ˆæ¤œç´¢ï¼‰
    results, debug = search_docs(
        user_query=user_input,
        docs=docs,
        word_vectorizer=word_vec,
        word_matrix=word_mat,
        char_vectorizer=char_vec,
        char_matrix=char_mat,
        vocab=VOCAB,
        top_k=TOP_K,
        alpha=ALPHA,
        min_score=MIN_SCORE,
    )

    # 3) å­¦ç¿’ç”¨ï¼šæ¤œç´¢ã«ä½¿ã‚ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆwordå´ï¼‰ã‚’è¡¨ç¤ºï¼ˆä¸è¦ãªã‚‰å‰Šé™¤OKï¼‰
    if debug.get("q_word"):
        st.caption(f"ğŸ” æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆwordå´ï¼‰: {debug['q_word']}")

    # 4) æ¤œç´¢çµæœãŒ0ä»¶ãªã‚‰ã€RAGã¯ã›ãšã€Œè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã€ã‚’è¿”ã™
    if not results:
        gemini_block = (
            "**ã€Geminiã®è¿”ç­”ï¼ˆRAGï¼‰ã€‘**\n\n"
            "é–¢é€£ã™ã‚‹è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€æ ¹æ‹ ã«åŸºã¥ãå›ç­”ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n"
            "ã‚‚ã†å°‘ã—å…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è³ªå•ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šæ—¥éŠ€ æ”¿ç­–é‡‘åˆ© æ®ãˆç½®ãï¼‰ã€‚\n"
        )
        tfidf_block = "ï¼ˆæ¤œç´¢çµæœï¼š0ä»¶ï¼‰"
        bot_reply = gemini_block + "\n---\n\n" + tfidf_block

    else:
        # 5) RAGãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼ˆæ ¹æ‹ ã‚’åŸ‹ã‚è¾¼ã‚€ï¼‰
        rag_prompt = build_rag_prompt(
            question=user_input,
            results=results,
            query_keywords=debug["q_word"] or "(æŠ½å‡ºãªã—ï¼šcharæ¤œç´¢ã®ã¿ã§ãƒ’ãƒƒãƒˆ)",
        )

        # 6) Geminiå‘¼ã³å‡ºã—ï¼ˆä¾‹å¤–ã§è½ã¡ãªã„ã‚ˆã†tryï¼‰
        try:
            rag_answer = gemini_ask(gemini_model, rag_prompt) or "(Geminiã®è¿”ç­”ãŒç©ºã§ã—ãŸ)"
            gemini_block = f"**ã€Geminiã®è¿”ç­”ï¼ˆRAGï¼šæ ¹æ‹ ã«åŸºã¥ãå›ç­”ï¼‰ã€‘**\n\n{rag_answer}\n"
        except Exception as e:
            gemini_block = f"**ã€Geminiã®è¿”ç­”ï¼ˆRAGï¼‰ã€‘**\n\nGeminiå‘¼ã³å‡ºã—ã§ã‚¨ãƒ©ãƒ¼: {e}\n"

        # 7) å‚ç…§è¨˜äº‹ï¼ˆé€æ˜æ€§ã®ãŸã‚ã€ã‚¿ã‚¤ãƒˆãƒ«/ã‚¹ã‚³ã‚¢/URLã‚’å‡ºã™ï¼‰
        lines = [f"**ã€å‚ç…§ã—ãŸè¨˜äº‹ï¼ˆä¸Šä½{len(results)}ä»¶ï¼‰ã€‘**\n"]
        for rank, r in enumerate(results, start=1):
            lines.append(
                f"{rank}. **{r['title']}**  \n"
                f"ã€€- topic: `{r['topic']}`  \n"
                f"ã€€- score: `{r['score']:.4f}`  \n"
                f"ã€€- url: {r['url']}\n"
            )
        tfidf_block = "\n".join(lines)

        # 8) è¿”ç­”ãƒ–ãƒ­ãƒƒã‚¯åˆæˆ
        bot_reply = gemini_block + "\n---\n\n" + tfidf_block

    # 9) ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆè¿”ç­”ã‚’å±¥æ­´ã«è¿½åŠ ã—ã€rerunã§ç”»é¢æ›´æ–°
    st.session_state.messages.append({"role": "assistant", "content": bot_reply})
    st.rerun()