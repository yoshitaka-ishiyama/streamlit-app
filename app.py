import streamlit as st

# ============================
# Step0: ã‚¢ãƒ—ãƒªåˆæœŸåŒ–ï¼ˆStreamlitè¨­å®šï¼‰
# ============================
st.set_page_config(page_title="RAG Chatbot (TF-IDF + Gemini)", layout="centered")

# ============================
# Step0: import
# ============================
import os
import re
import pandas as pd
import numpy as np

from dotenv import load_dotenv
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai

# ============================
# Step0: æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå›ºå®šã§OKï¼‰
# ============================
ALPHA = 0.6          # çµ±åˆã‚¹ã‚³ã‚¢ã®é‡ã¿ï¼šwordå¯„ã‚Š=1.0 / charå¯„ã‚Š=0.0
TOP_K = 3            # å‚ç…§ã™ã‚‹è¨˜äº‹æ•°
MIN_SCORE = 0.05     # ã“ã‚Œæœªæº€ã¯ã€Œãƒ’ãƒƒãƒˆãªã—ã€æ‰±ã„ï¼ˆåºƒã™ãã‚‹è³ªå•ã®èª¤ãƒ’ãƒƒãƒˆæŠ‘åˆ¶ï¼‰

# ============================
# Step0: .env èª­ã¿è¾¼ã¿ â†’ APIã‚­ãƒ¼ç¢ºèª
# ============================
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    st.error("ç’°å¢ƒå¤‰æ•° GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ============================
# Step1: Gemini APIï¼ˆLLMï¼‰æº–å‚™
# ============================
@st.cache_resource
def get_gemini_model(api_key: str):
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-2.5-flash-lite")

def gemini_ask(model, prompt: str) -> str:
    prompt = (prompt or "").strip()
    if not prompt:
        return ""
    resp = model.generate_content(prompt)
    return getattr(resp, "text", "") or ""

# ============================
# Step2: CSVèª­è¾¼ â†’ docs ä½œæˆ
# ============================
@st.cache_data
def load_news_csv(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path)

    required_cols = ["topic", "url", "title", "text", "text_tokenized"]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        raise ValueError(f"CSVã«å¿…è¦ãªåˆ—ãŒã‚ã‚Šã¾ã›ã‚“: {missing}. columns={list(df.columns)}")

    df["topic"] = df["topic"].fillna("")
    df["url"] = df["url"].fillna("")
    df["title"] = df["title"].fillna("")
    df["text"] = df["text"].fillna("")
    df["text_tokenized"] = df["text_tokenized"].fillna("")
    return df

def build_docs(df: pd.DataFrame) -> list[dict]:
    docs: list[dict] = []
    for i, row in df.reset_index(drop=True).iterrows():
        title = str(row["title"]).strip()
        text = str(row["text"]).strip()

        docs.append(
            {
                "doc_id": int(i),
                "topic": str(row["topic"]),
                "url": str(row["url"]),
                "title": title,
                "text": text,
                # wordæ¤œç´¢ç”¨ï¼ˆCSVå´ãŒåˆ†ã‹ã¡æ›¸ãæ¸ˆã¿å‰æï¼‰
                "tfidf_word": str(row["text_tokenized"]),
                # charæ¤œç´¢ç”¨ï¼ˆè‡ªç„¶æ–‡ã«å¼·ã„ï¼šã‚¿ã‚¤ãƒˆãƒ«ã‚‚æ··ãœã‚‹ï¼‰
                "tfidf_char": f"{title} {text}",
                # Geminiã«æ¸¡ã™æ ¹æ‹ 
                "display_text": f"ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘{title}\n\nã€æœ¬æ–‡ã€‘\n{text}",
            }
        )
    return docs

# ============================
# Step3: TF-IDF ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆwordç‰ˆ / charç‰ˆï¼‰
# ============================
@st.cache_data
def build_tfidf_indexes(docs: list[dict]):
    # ---- wordï¼ˆåˆ†ã‹ã¡æ›¸ãå‰æï¼‰
    word_corpus = [d["tfidf_word"] for d in docs]
    word_vectorizer = TfidfVectorizer(
        tokenizer=str.split,
        preprocessor=None,
        token_pattern=None,
    )
    word_matrix = word_vectorizer.fit_transform(word_corpus)

    # ---- charï¼ˆè‡ªç„¶æ–‡ãƒ»è¡¨è¨˜ã‚†ã‚Œã«å¼·ã„ï¼‰
    char_corpus = [d["tfidf_char"] for d in docs]
    char_vectorizer = TfidfVectorizer(
        analyzer="char_wb",
        ngram_range=(2, 4),
        min_df=2,
    )
    char_matrix = char_vectorizer.fit_transform(char_corpus)

    return word_vectorizer, word_matrix, char_vectorizer, char_matrix

# ============================
# Step3.5: è³ªå•æ–‡ã‚’TF-IDFå‘ã‘ã«ç°¡æ˜“æ­£è¦åŒ–ï¼ˆæ±ç”¨ç‰ˆï¼‰
# - vocabï¼ˆwordå´ã®èªå½™ï¼‰ã«å¯„ã›ã‚‹
# ============================
_CHUNK_RE = re.compile(r"[ä¸€-é¾¥]+|[ã-ã‚“]+|[ã‚¡-ãƒ³]+|[A-Za-z0-9]+")

def normalize_query_for_tfidf(query: str, vocab: set[str]) -> str:
    """
    è‡ªç„¶æ–‡ã‚¯ã‚¨ãƒªã‚’ã€CSVå´(text_tokenized)ã®èªå½™ã«å¯„ã›ãŸã€Œã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šãƒˆãƒ¼ã‚¯ãƒ³ã€ã«ã™ã‚‹ï¼ˆæ±ç”¨ç‰ˆï¼‰
    - å½¢æ…‹ç´ è§£æãªã—
    - èªå½™(vocab)ã«å­˜åœ¨ã™ã‚‹èªã ã‘æ®‹ã™
    - vocabã«ç„¡ã„è¤‡åˆèªã¯ã€vocabã«è¼‰ã‚‹ã‚ˆã†ã«åˆ†å‰²ã§ãã‚‹ãªã‚‰åˆ†å‰²èªã‚’è¿½åŠ 
    """
    q = (query or "").strip()
    if not q:
        return ""

    # è¨˜å·ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã¸
    q = re.sub(r"[^\wã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]+", " ", q)

    # ã–ã£ãã‚Šåˆ†å‰²ï¼ˆæ¼¢å­—ã®å¡Š / ã²ã‚‰ãŒãª / ã‚«ã‚¿ã‚«ãƒŠ / è‹±æ•°å­—ï¼‰
    chunks = _CHUNK_RE.findall(q)

    # æœ€ä½é™ã®ã‚¹ãƒˆãƒƒãƒ—ï¼ˆåŠ©è©ãƒ»å®šå‹ï¼‰
    stop = {
        "ã«ã¤ã„ã¦","æ•™ãˆã¦","ãã ã•ã„","ã¨ã¯","ã§ã™","ã¾ã™","ã™ã‚‹","ã—ãŸã„",
        "ã®","ãŒ","ã‚’","ã«","ã¸","ã¨","ã‚‚","ã‚„","ã‹ã‚‰","ã¾ã§","ã‚ˆã‚Š",
        "ã§ã™ã‹","ã¾ã™ã‹","ã‚ã‚‹","ã‚ã‚Šã¾ã™","ã„ã‚‹","ã„ã¾ã™",
        "æœ€è¿‘","ä½•","ã©ã‚“ãª","ã©ã†"
    }

    # ã¾ãšã¯ãã®ã¾ã¾å€™è£œåŒ–
    candidates = []
    for c in chunks:
        if c in stop:
            continue
        candidates.append(c)

    # vocabã«ç„¡ã„èªã¯ã€Œèªå½™ã«è¼‰ã‚‹ã‚ˆã†ã«åˆ†å‰²ã§ãã‚‹ã‹ã€ã‚’è©¦ã™ï¼ˆæ±ç”¨ï¼‰
    expanded = []
    for t in candidates:
        expanded.append(t)

        # vocabã«ç„¡ãã€ã‚ã‚‹ç¨‹åº¦é•·ã„å ´åˆã ã‘åˆ†å‰²æ¢ç´¢
        if (t not in vocab) and (len(t) >= 4):
            for i in range(2, len(t) - 1):
                a, b = t[:i], t[i:]
                if (a in vocab) and (b in vocab):
                    expanded.extend([a, b])

    # vocabã«å­˜åœ¨ã™ã‚‹èªã ã‘æ®‹ã™ï¼ˆã“ã“ãŒé‡è¦ï¼‰
    expanded = [t for t in expanded if t in vocab]

    # é‡è¤‡é™¤å»ï¼ˆé †åºç¶­æŒï¼‰
    seen = set()
    uniq = []
    for t in expanded:
        if t not in seen:
            uniq.append(t)
            seen.add(t)

    return " ".join(uniq)

# ============================
# Step4: æ¤œç´¢ï¼ˆçµ±åˆã‚¹ã‚³ã‚¢ã§ä¸Šä½æŠ½å‡ºï¼‰
# ============================
def search_docs(
    user_query: str,
    docs: list[dict],
    word_vectorizer,
    word_matrix,
    char_vectorizer,
    char_matrix,
    vocab: set[str],
    top_k: int = 3,
    alpha: float = 0.6,
    min_score: float = 0.05,
):
    user_query = (user_query or "").strip()
    if not user_query:
        return [], {"q_word": "", "max_score": 0.0}

    # wordç”¨ï¼ˆèªå½™å¯„ã›ã®ç°¡æ˜“æ­£è¦åŒ–ï¼‰
    q_word = normalize_query_for_tfidf(user_query, vocab)

    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    qv_char = char_vectorizer.transform([user_query])
    char_scores = cosine_similarity(qv_char, char_matrix).flatten()

    if q_word:
        qv_word = word_vectorizer.transform([q_word])
        word_scores = cosine_similarity(qv_word, word_matrix).flatten()
        combined = alpha * word_scores + (1 - alpha) * char_scores
    else:
        # wordãŒä½œã‚Œãªã„ï¼èªå½™ã«å¯„ã‚‰ãªã„è‡ªç„¶æ–‡ãªã®ã§ã€charã‚’100%æ¡ç”¨ï¼ˆè–„ã‚ãªã„ï¼‰
        combined = char_scores

    # ä¸Šä½
    top_idx = np.argsort(combined)[::-1][:top_k]

    # é–¾å€¤æœªæº€ã¯æ¨ã¦ã‚‹ï¼ˆèª¤ãƒ’ãƒƒãƒˆæŠ‘åˆ¶ï¼‰
    top_idx = [i for i in top_idx if combined[i] >= min_score]

    results = []
    for i in top_idx:
        results.append(
            {
                "doc_id": docs[i]["doc_id"],
                "title": docs[i]["title"],
                "url": docs[i]["url"],
                "topic": docs[i]["topic"],
                "score": float(combined[i]),
                "display_text": docs[i]["display_text"],
            }
        )

    debug = {"q_word": q_word, "max_score": float(combined.max()) if combined.size else 0.0}
    return results, debug

# ============================
# Step5: RAGï¼ˆæ ¹æ‹ ä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
# ============================
def build_rag_prompt(question: str, results: list[dict], query_keywords: str) -> str:
    evidence_blocks = []
    for i, r in enumerate(results, start=1):
        evidence = (r.get("display_text") or "")[:2000]
        evidence_blocks.append(f"[æ ¹æ‹ {i}] topic={r.get('topic')} url={r.get('url')}\n{evidence}")

    evidence_text = "\n\n---\n\n".join(evidence_blocks)

    prompt = f"""
ã‚ãªãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®è¦ç´„ãƒ»è§£èª¬ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æ¬¡ã®ã€æ ¹æ‹ ã€‘ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã ã‘ã‚’ä½¿ã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
æ ¹æ‹ ã«æ›¸ã‹ã‚Œã¦ã„ãªã„ã“ã¨ã¯æ¨æ¸¬ã›ãšã€ã€Œæ ¹æ‹ ä¸è¶³ã€ã¨æ˜ç¢ºã«ä¼ãˆã¦ãã ã•ã„ã€‚

# è³ªå•
{question}

# è³ªå•ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå‚è€ƒï¼‰
{query_keywords}

# æ ¹æ‹ 
{evidence_text}

# æŒ‡ç¤º
- ã¾ãšçµè«–ã‚’1ã€œ2è¡Œ
- æ¬¡ã«æ ¹æ‹ ã«åŸºã¥ãèª¬æ˜ï¼ˆç®‡æ¡æ›¸ãã§OKï¼‰
- æœ€å¾Œã«ã€Œå‚ç…§ã—ãŸæ ¹æ‹ ç•ªå·ï¼ˆä¾‹ï¼š[æ ¹æ‹ 1][æ ¹æ‹ 3]ï¼‰ã€ã‚’ä»˜ã‘ã‚‹
""".strip()

    return prompt

# ============================
# Step2ã€œStep5: èµ·å‹•æ™‚ã«ä¸€åº¦ã ã‘æº–å‚™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚åŠ¹ãï¼‰
# ============================
CSV_PATH = "dataset/yahoo_news_articles_preprocessed.csv"

df_news = load_news_csv(CSV_PATH)
docs = build_docs(df_news)

word_vec, word_mat, char_vec, char_mat = build_tfidf_indexes(docs)
gemini_model = get_gemini_model(GOOGLE_API_KEY)

# â˜…é‡è¦ï¼šwordå´ã®èªå½™ã‚’ä½œã‚‹ï¼ˆã‚ãªãŸã®å…¨æ–‡ã®ãƒã‚°ä¿®æ­£ç‚¹ï¼‰
VOCAB = set(word_vec.get_feature_names_out())

# ============================
# Step6: UIï¼ˆãƒãƒ£ãƒƒãƒˆï¼‰
# ============================
st.title("ğŸ—¨ï¸ ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ãƒãƒ£ãƒƒãƒˆï¼ˆTF-IDF + Gemini RAGï¼‰")
st.caption(f"æ¤œç´¢è¨­å®šï¼ˆå›ºå®šï¼‰ï¼šalpha={ALPHA} / TOP_K={TOP_K} / MIN_SCORE={MIN_SCORE}")

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_input = st.chat_input("ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šæ—¥éŠ€ã®æ”¿ç­–é‡‘åˆ©ã«ã¤ã„ã¦æ•™ãˆã¦ï¼‰")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})

    # 1) Retrieval
    results, debug = search_docs(
        user_query=user_input,
        docs=docs,
        word_vectorizer=word_vec,
        word_matrix=word_mat,
        char_vectorizer=char_vec,
        char_matrix=char_mat,
        vocab=VOCAB,
        top_k=TOP_K,
        alpha=ALPHA,
        min_score=MIN_SCORE,
    )

    # å­¦ç¿’ç”¨ï¼šæ¤œç´¢ã«ä½¿ã‚ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆwordå´ï¼‰ã ã‘è¡¨ç¤ºï¼ˆä¸è¦ãªã‚‰å‰Šé™¤OKï¼‰
    if debug["q_word"]:
        st.caption(f"ğŸ” æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆwordå´ï¼‰: {debug['q_word']}")

    # 2) RAGå›ç­”
    if not results:
        gemini_block = (
            "**ã€Geminiã®è¿”ç­”ï¼ˆRAGï¼‰ã€‘**\n\n"
            "é–¢é€£ã™ã‚‹è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€æ ¹æ‹ ã«åŸºã¥ãå›ç­”ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n"
            "ã‚‚ã†å°‘ã—å…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è³ªå•ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šæ—¥éŠ€ æ”¿ç­–é‡‘åˆ© æ®ãˆç½®ãï¼‰ã€‚\n"
        )
        tfidf_block = "ï¼ˆæ¤œç´¢çµæœï¼š0ä»¶ï¼‰"
        bot_reply = gemini_block + "\n---\n\n" + tfidf_block
    else:
        rag_prompt = build_rag_prompt(
            question=user_input,
            results=results,
            query_keywords=debug["q_word"] or "(æŠ½å‡ºãªã—ï¼šcharæ¤œç´¢ã®ã¿ã§ãƒ’ãƒƒãƒˆ)",
        )

        try:
            rag_answer = gemini_ask(gemini_model, rag_prompt) or "(Geminiã®è¿”ç­”ãŒç©ºã§ã—ãŸ)"
            gemini_block = f"**ã€Geminiã®è¿”ç­”ï¼ˆRAGï¼šæ ¹æ‹ ã«åŸºã¥ãå›ç­”ï¼‰ã€‘**\n\n{rag_answer}\n"
        except Exception as e:
            gemini_block = f"**ã€Geminiã®è¿”ç­”ï¼ˆRAGï¼‰ã€‘**\n\nGeminiå‘¼ã³å‡ºã—ã§ã‚¨ãƒ©ãƒ¼: {e}\n"

        # 3) å‚ç…§è¨˜äº‹
        lines = [f"**ã€å‚ç…§ã—ãŸè¨˜äº‹ï¼ˆä¸Šä½{len(results)}ä»¶ï¼‰ã€‘**\n"]
        for rank, r in enumerate(results, start=1):
            lines.append(
                f"{rank}. **{r['title']}**  \n"
                f"ã€€- topic: `{r['topic']}`  \n"
                f"ã€€- score: `{r['score']:.4f}`  \n"
                f"ã€€- url: {r['url']}\n"
            )
        tfidf_block = "\n".join(lines)

        bot_reply = gemini_block + "\n---\n\n" + tfidf_block

    st.session_state.messages.append({"role": "assistant", "content": bot_reply})
    st.rerun()